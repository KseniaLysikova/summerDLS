{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VO9HIvIeLLii"
   },
   "source": [
    "SincNet основана на функциях sinc, которые реализуют полосовые фильтры. В отличие от стандартных CNN в SincNet непосредственно из данных изучаются только низкие и высокие частоты среза.\n",
    "Низкая и высокая частоты среза являются единственными параметрами фильтра, обучаемыми из данных\n",
    "Сеть начинается с SincNet слоя, за которым следуют стандартные слои CNN и завершается полносвязными слоями для классификации.\n",
    "![image.png](attachment:36eb2123-5fdd-4ff7-86c6-ae85ba225a40.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fbu7we-B2o3M",
    "outputId": "e863b755-44ce-47be-b189-723f6521f53d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/awsaf49/asvpoof-2019-dataset\n",
      "License(s): ODC Attribution License (ODC-By)\n",
      "Downloading asvpoof-2019-dataset.zip to /content\n",
      " 19% 4.49G/23.6G [03:56<15:24, 22.2MB/s]"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d awsaf49/asvpoof-2019-dataset\n",
    "!unzip asvpoof-2019-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzKlXwJr27pV"
   },
   "outputs": [],
   "source": [
    "!sudo rm -rf /content/PA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7kl8ruV4qMG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OU4TnnU5JeJ"
   },
   "outputs": [],
   "source": [
    "train_audio_folder = 'C:/Users/Ksenia/Desktop/content/LA/LA/ASVspoof2019_LA_train/flac'\n",
    "train_metadata_file = 'C:/Users/Ksenia/Desktop/content/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2YhZQA7q29oF"
   },
   "outputs": [],
   "source": [
    "test_audio_folder = 'C:/Users/Ksenia/Desktop/content/LA/LA/ASVspoof2019_LA_eval/flac'\n",
    "test_metadata_file = 'C:/Users/Ksenia/Desktop/content/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s762UC0A5OJC"
   },
   "outputs": [],
   "source": [
    "train_metadata = pd.read_csv(train_metadata_file, sep=' ', header=None)\n",
    "train_metadata.columns = ['speaker', 'filename', 'sep1', 'sep2', 'label']\n",
    "train_metadata = train_metadata[['filename', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bvLfzpF43W8G"
   },
   "outputs": [],
   "source": [
    "test_metadata = pd.read_csv(test_metadata_file, sep=' ', header=None)\n",
    "test_metadata.columns = ['speaker', 'filename', 'sep1', 'sep2', 'label']\n",
    "test_metadata = test_metadata[['filename', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDEUm6cN5XUg"
   },
   "outputs": [],
   "source": [
    "train_audio_file_names = [f\"{file_name}.flac\" for file_name in train_metadata['filename'].tolist()]\n",
    "train_labels = train_metadata['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1m67X2cs3oC8"
   },
   "outputs": [],
   "source": [
    "test_audio_file_names = [f\"{file_name}.flac\" for file_name in test_metadata['filename'].tolist()]\n",
    "test_labels = test_metadata['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGhdf3poiQWj"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_labels)\n",
    "test_labels = label_encoder.fit_transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBQNhdq53GLG"
   },
   "outputs": [],
   "source": [
    "class AudioMNISTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio_dir_path, audio_file_names, num_samples, labels):\n",
    "        super().__init__()\n",
    "        self.audio_dir_path = audio_dir_path\n",
    "        self.audio_file_names = audio_file_names\n",
    "        self.num_samples = num_samples\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = os.path.join(self.audio_dir_path, self.audio_file_names[index])\n",
    "        signal, sr = torchaudio.load(path.replace(\"\\\\\", \"/\"))\n",
    "        signal = self.mix_down_if_necessary(signal)\n",
    "        signal = self.cut_if_necessary(signal)\n",
    "        signal = self.right_pad_if_necessary(signal)\n",
    "        label = (self.labels[index])\n",
    "        return signal, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim = 0, keepdims = True)\n",
    "        return signal\n",
    "\n",
    "    def cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :num_samples]\n",
    "        return signal\n",
    "\n",
    "    def right_pad_if_necessary(self, signal):\n",
    "        length = signal.shape[1]\n",
    "        if self.num_samples > length:\n",
    "            pad_last_dim = (0, num_samples - length)\n",
    "            signal = torch.nn.functional.pad(signal, pad_last_dim)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hAoUiIN05mkR"
   },
   "outputs": [],
   "source": [
    "num_samples = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DsuLUY3H4Z9i"
   },
   "outputs": [],
   "source": [
    "train_dataset = AudioMNISTDataset(train_audio_folder, train_audio_file_names, num_samples, train_labels)\n",
    "test_dataset = AudioMNISTDataset(test_audio_folder, test_audio_file_names, num_samples, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJUkok85slgB"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle = True, batch_size = 128)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle = False, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rjc6crCWsWDq"
   },
   "outputs": [],
   "source": [
    "class SincConv(nn.Module):\n",
    "    def __init__(self, out_channels, kernel_size, sample_rate):\n",
    "        super(SincConv, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.sample_rate = sample_rate\n",
    "        self.band_pass = nn.Parameter(torch.Tensor(out_channels, 2))\n",
    "        self.init_kernels()\n",
    "\n",
    "    def init_kernels(self):\n",
    "        self.band_pass.data[:, 0] = torch.linspace(30, 300, self.out_channels)\n",
    "        self.band_pass.data[:, 1] = torch.linspace(3000, 8000, self.out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        filters = self.create_filters()\n",
    "        return nn.functional.conv1d(x, filters, stride=1, padding=self.kernel_size//2)\n",
    "\n",
    "    def create_filters(self):\n",
    "        filters = torch.zeros(self.out_channels, 1, self.kernel_size)\n",
    "        for i in range(self.out_channels):\n",
    "            low, high = self.band_pass[i]\n",
    "            filters[i, 0, :] = self.sinc_filter(low, high)\n",
    "        return filters\n",
    "\n",
    "    def sinc_filter(self, low, high):\n",
    "        t = torch.linspace(-self.kernel_size//2, self.kernel_size//2, self.kernel_size)\n",
    "        t = t.detach().numpy()\n",
    "        sinc_filter = (np.sin(2 * np.pi * high.item() * t) - np.sin(2 * np.pi * low.item() * t)) / (np.pi * t)\n",
    "        sinc_filter[t == 0] = 2 * (high.item() - low.item())\n",
    "        window = 0.54 - 0.46 * np.cos(2 * np.pi * np.arange(self.kernel_size) / (self.kernel_size - 1))\n",
    "        return torch.from_numpy(sinc_filter * window).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAbrn-_UssnF"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOkOkcdZsu_C"
   },
   "outputs": [],
   "source": [
    "class SincNetResNet(nn.Module):\n",
    "    def __init__(self, kernel_size, sample_rate, resnet_blocks, sinc_out_channels=20):\n",
    "        super(SincNetResNet, self).__init__()\n",
    "        self.sinc_conv = SincConv(sinc_out_channels, kernel_size, sample_rate)\n",
    "        self.resnet_blocks = nn.Sequential(*[BasicBlock(sinc_out_channels, sinc_out_channels) for _ in range(resnet_blocks)])\n",
    "        self.fc = nn.Linear(sinc_out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sinc_conv(x)\n",
    "        x = self.resnet_blocks(x)\n",
    "        x = torch.mean(x, dim=-1)\n",
    "        x = self.fc(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFQUjFpdA-oL"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_stochastic(model, loader, criterion, optimizer, num_epoch):\n",
    "    for epoch in tqdm(range(num_epoch)):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        epoch_loss = []\n",
    "\n",
    "        for X_batch, y_batch in loader:\n",
    "\n",
    "            y_batch = y_batch.float().unsqueeze(1)\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "            y_true.append(y_batch.detach().cpu().numpy())\n",
    "            y_pred.append(outputs.detach().cpu().numpy())\n",
    "\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            epoch_loss.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        y_true = np.concatenate(y_true)\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "        train_eer = EER(y_true, y_pred)\n",
    "        print(f'Epoch {epoch+1}, EER: {train_eer}')\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {np.mean(epoch_loss)}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkU3TVTnWiNs"
   },
   "outputs": [],
   "source": [
    "def EER(labels, outputs):\n",
    "    fpr, tpr, threshold = roc_curve(labels, outputs, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_threshold\n",
    "    eer = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    return eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLk5Ywc0EQVO",
    "outputId": "52f1bda9-3282-4186-b70d-470e5507ea09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9l87fKTI4qG5"
   },
   "outputs": [],
   "source": [
    "kernel_size = 251\n",
    "sample_rate = 16000\n",
    "resnet_blocks = 3\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "model = SincNetResNet(kernel_size, sample_rate, resnet_blocks)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "b6eb4f6704ea4bb580e655076a7e9175"
     ]
    },
    "id": "rEFJKcFeOPvo",
    "outputId": "3bbe03e1-b593-41fb-8524-190d262614ed"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6eb4f6704ea4bb580e655076a7e9175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, EER: 0.29496124031007753\n",
      "Epoch 1, Loss: 0.2799726954506869\n",
      "Epoch 2, EER: 0.14534883720930233\n",
      "Epoch 2, Loss: 0.20754224822000045\n",
      "Epoch 3, EER: 0.14186046511627906\n",
      "Epoch 3, Loss: 0.19375415689232361\n",
      "Epoch 4, EER: 0.12829457364341085\n",
      "Epoch 4, Loss: 0.1794133602674283\n",
      "Epoch 5, EER: 0.12248062015503876\n",
      "Epoch 5, Loss: 0.17163822717552807\n",
      "Epoch 6, EER: 0.11976744186046512\n",
      "Epoch 6, Loss: 0.16738982120500737\n",
      "Epoch 7, EER: 0.11317829457364341\n",
      "Epoch 7, Loss: 0.16208714246749878\n",
      "Epoch 8, EER: 0.11356589147286822\n",
      "Epoch 8, Loss: 0.16042103614639397\n",
      "Epoch 9, EER: 0.11124031007751937\n",
      "Epoch 9, Loss: 0.1587205512364905\n",
      "Epoch 10, EER: 0.1127906976744186\n",
      "Epoch 10, Loss: 0.15913340739493992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SincNetResNet(\n",
       "  (sinc_conv): SincConv()\n",
       "  (resnet_blocks): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=20, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stochastic(model, train_loader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbsnAMirCt9l"
   },
   "outputs": [],
   "source": [
    "new_outputs = []\n",
    "new_labels = []\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        outputs = model(X)\n",
    "\n",
    "        new_outputs.append(y.detach().cpu().numpy())\n",
    "        new_labels.append(outputs.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1C1AFNwLLir",
    "outputId": "4fed838e-ffa8-4fb2-9308-47e7a5188112"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71237, 1) (71237,)\n"
     ]
    }
   ],
   "source": [
    "labels = np.concatenate(new_labels)\n",
    "outputs = np.concatenate(new_outputs)\n",
    "print(labels.shape, outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "20oZbBBTCx1z",
    "outputId": "644f7112-4709-402e-c7b3-6aff878ae787"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5420320532204415"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EER(labels, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkLZs7g1LLir"
   },
   "outputs": [],
   "source": [
    "kernel_size = 251\n",
    "sample_rate = 16000\n",
    "resnet_blocks = 3\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "model2 = SincNetResNet(kernel_size, sample_rate, resnet_blocks, 50)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "1e06f53866734bba85f9ab64d4be5f35"
     ]
    },
    "id": "vGmjb3NjLLis",
    "outputId": "565393b1-6057-4f42-acbe-8500c520780d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e06f53866734bba85f9ab64d4be5f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, EER: 0.2957364341085271\n",
      "Epoch 1, Loss: 0.2748691032280275\n",
      "Epoch 2, EER: 0.14573643410852713\n",
      "Epoch 2, Loss: 0.20665016534490202\n",
      "Epoch 3, EER: 0.1387596899224806\n",
      "Epoch 3, Loss: 0.19543003818797106\n",
      "Epoch 4, EER: 0.12906976744186047\n",
      "Epoch 4, Loss: 0.1797170125734267\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m     epoch_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 17\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     20\u001b[0m y_true \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(y_true)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(num_epochs)):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        epoch_loss = []\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "\n",
    "            y_batch = y_batch.float().unsqueeze(1)\n",
    "\n",
    "            outputs = model2(X_batch)\n",
    "            y_true.append(y_batch.detach().cpu().numpy())\n",
    "            y_pred.append(outputs.detach().cpu().numpy())\n",
    "\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            epoch_loss.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        y_true = np.concatenate(y_true)\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "        train_eer = EER(y_true, y_pred)\n",
    "        print(f'Epoch {epoch+1}, EER: {train_eer}')\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {np.mean(epoch_loss)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xdj3kGyVLLis"
   },
   "source": [
    "Думала может получше будет, но очень много времени заняло, а результат пока не сильно отличается, значит напортачила я тут эхэх"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1hLw6OfLO4N"
   },
   "source": [
    "GitHub: https://github.com/mravanelli/SincNet"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
